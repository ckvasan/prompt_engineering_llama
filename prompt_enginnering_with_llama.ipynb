{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b18ddac6",
   "metadata": {},
   "source": [
    " ## Note :\n",
    " \n",
    " - Regsiter in together.ai\n",
    " - Open the util.py and update your api key\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe14384",
   "metadata": {},
   "source": [
    "# Import helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca9fecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import llama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "386330af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining prompt\n",
    "\n",
    "prompt = \"Help me in writing welcome note to newly joined team member John\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55478d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llama(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb8daee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': {'message': 'Invalid API key provided. You can find your API key at https://api.together.xyz/settings/api-keys.',\n",
       "  'type': 'invalid_request_error',\n",
       "  'param': None,\n",
       "  'code': 'invalid_api_key'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "932bf4df",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Markdown expects text, not {'error': {'message': 'Invalid API key provided. You can find your API key at https://api.together.xyz/settings/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, Markdown\n\u001b[1;32m----> 2\u001b[0m display(Markdown(response))\n",
      "File \u001b[1;32mD:\\AI\\Software\\anaconda3\\Lib\\site-packages\\IPython\\core\\display.py:328\u001b[0m, in \u001b[0;36mDisplayObject.__init__\u001b[1;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreload()\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_data()\n",
      "File \u001b[1;32mD:\\AI\\Software\\anaconda3\\Lib\\site-packages\\IPython\\core\\display.py:407\u001b[0m, in \u001b[0;36mTextDisplayObject._check_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 407\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expects text, not \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata))\n",
      "\u001b[1;31mTypeError\u001b[0m: Markdown expects text, not {'error': {'message': 'Invalid API key provided. You can find your API key at https://api.together.xyz/settings/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4a4bfb",
   "metadata": {},
   "source": [
    "# Types of Models\n",
    "\n",
    "# Base Model\n",
    "- helps providing the next token of prompt \n",
    "\n",
    "\n",
    "# Instruction Model\n",
    "- It is chat based converstaion model/ provide solution to prompt\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd51ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### base model\n",
    "# token is simply 3/4 of the word\n",
    "# It predicts next token of prompt, example it predicts the next word \"flowers/plants\"\n",
    "prompt = \"The garden is full of\"\n",
    "response = llama(prompt, \n",
    "                 verbose=True,\n",
    "                 add_inst=False,\n",
    "                 model=\"togethercomputer/llama-2-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f0b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545f0990",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Instruction model,\n",
    "# It should be sorrounded by [INST] [/INST] tags\n",
    "# token is simply 3/4 of the word\n",
    "# It predicts solution to prompt\"\n",
    "prompt = \"what is captial of germany? \"\n",
    "response = llama(prompt, \n",
    "                 verbose=True,\n",
    "                 model=\"togethercomputer/llama-2-7b-chat\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ceac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Base model,\n",
    "\n",
    "# It predicts next token of the prompt, \n",
    "# here it can't predict the next to the prompt, hence repeatly printinhg\n",
    "\n",
    "prompt = \"what is captial of germany? \"\n",
    "response = llama(prompt, \n",
    "                 verbose=True,\n",
    "                 add_inst=False,\n",
    "                 model=\"togethercomputer/llama-2-7b\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7cf7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689f824a",
   "metadata": {},
   "source": [
    " ### Note : Most used model is Instruction model in LLM Application like ChatGPT, Gemma,Mistral AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56e0c8e",
   "metadata": {},
   "source": [
    "### Changing the temperature setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24998105",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Help me write a birthday card for my dear friend John.\n",
    "Here are details about my friend:\n",
    "He likes football.\n",
    "His hobbies bike riding.\n",
    "His favorite color is light blue.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4dcedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting temperature =  0.0, model response is noraml/mostly expected\n",
    "response = llama(prompt, temperature=0.9)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbc678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setting temperature =  1, model response is funny\n",
    "\n",
    "response = llama(prompt, temperature=0.9)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b771e5b",
   "metadata": {},
   "source": [
    "### Changing the max tokens setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad3df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max tokens provides output result with maximun tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01116eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llama(prompt, temperature=0.0,max_tokens=30)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llama(prompt, temperature=0.0,max_tokens=250)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f6d157",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Help me write a birthday card for my dear friend Andrew.\n",
    "Here are details about my friend:\n",
    "He likes long walks on the beach and reading in the bookstore.\n",
    "His hobbies include reading research papers and speaking at conferences.\n",
    "His favorite color is light blue.\n",
    "He likes pandas.\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a5d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = \"\"\"\n",
    "Oh, he also likes teaching. Can you rewrite it to include that?\n",
    "\"\"\"\n",
    "response_2 = llama(prompt_2)\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e2fe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = \"\"\"\n",
    "what is my friend name?\n",
    "\"\"\"\n",
    "response_2 = llama(prompt_2)\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2136bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = \"\"\"\n",
    "Hi My friend name is Keny\n",
    "\"\"\"\n",
    "response_2 = llama(prompt_2)\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a72dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = \"\"\"\n",
    "what is my friend name?\n",
    "\"\"\"\n",
    "response_2 = llama(prompt_2)\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c41fc17",
   "metadata": {},
   "source": [
    "# Multi-turn conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d64d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    What are fun activities I can do this weekend?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb80e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = \"\"\"\n",
    "Which of these would be good for my health?\n",
    "\"\"\"\n",
    "response_2 = llama(prompt_2)\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f0c26",
   "metadata": {},
   "source": [
    "#### from the above ,LLM are stateless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c8ca54",
   "metadata": {},
   "source": [
    "# Constriction of Multi-turn conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9249fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import llama_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284ae491",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = \"\"\"\n",
    "    What are fun activities I can do this weekend?\n",
    "\"\"\"\n",
    "response_1 = llama(prompt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b07bdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = \"\"\"\n",
    "Which of these would be good for my health?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6538114",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = f\"\"\"\n",
    "<s>[INST] {prompt_1} [/INST]\n",
    "{response_1}\n",
    "</s>\n",
    "<s>[INST] {prompt_2} [/INST]\n",
    "\"\"\"\n",
    "print(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff800fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_2 = llama(chat_prompt,\n",
    "                 add_inst=False,\n",
    "                 verbose=True)\n",
    "response_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bindcoversation(prompts,responses) :\n",
    "    conversion = \"\"\n",
    "    for i in range(len(prompts)):\n",
    "        if  i== len(prompts)-1 :\n",
    "            conversion += f\"<s>[INST] {prompts[i]} [/INST]\"\n",
    "            return conversion\n",
    "        conversion += f\"<s>[INST] {prompts[i]} [/INST] {responses[i]} </s> \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a616381",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_3 = \"Which of these activites would be fun with friends?\"\n",
    "# invoke thru normal llma helper function\n",
    "prompts = [prompt_1, prompt_2, prompt_3]\n",
    "responses = [response_1, response_2]\n",
    "\n",
    "print(bindcoversation(prompts,responses))\n",
    "response_3 = llama(bindcoversion(prompts,responses), verbose=True)\n",
    "\n",
    "print(response_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becbb1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace prompt_3 with your own question!\n",
    "# invoke thru llma_chat helper function\n",
    "prompt_3 = \"Which of these activites would be fun with friends?\"\n",
    "prompts = [prompt_1, prompt_2, prompt_3]\n",
    "responses = [response_1, response_2]\n",
    "\n",
    "print(bindcoversion(prompts,responses))\n",
    "response_3 = llama_chat(prompts,responses, verbose=True)\n",
    "\n",
    "print(response_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3a5cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normal Prompting\n",
    "prompt = \"\"\"\n",
    "What is the sentiment of:\n",
    "Hi Amit, thanks for the thoughtful birthday card!\n",
    "\n",
    "Output : Either Positive or Negative \n",
    "\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2737e1",
   "metadata": {},
   "source": [
    "# Prompt Enginerring Technique\n",
    "### Zero shot prompting\n",
    "### One Shot prompting\n",
    "### Few Shot prompting\n",
    "### Chain of Thought Prompting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d164d631",
   "metadata": {},
   "source": [
    "## Zero Shot\n",
    "- No Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab4b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa9049",
   "metadata": {},
   "source": [
    "## One Shot Prompting\n",
    "- With one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b1ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebd328a",
   "metadata": {},
   "source": [
    "### Few Shot\n",
    "- Here is an example of few-shot prompting.\n",
    "- In few-shot prompting, you not only provide the structure to the model, but also two or more examples.\n",
    "- You are prompting the model to see if it can infer the task from the structure, as well as the examples in your prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b45b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b913bf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7B model may doesn't provide the response all the time\n",
    "\n",
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\n",
    "Give a one word response.\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e05b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70B model may provide better response compare to 7b\n",
    "\n",
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\n",
    "Give a one word response.\n",
    "\"\"\"\n",
    "response = llama(prompt,\n",
    "                model=\"togethercomputer/llama-2-70b-chat\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54b9684",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: \n",
    "\n",
    "Respond with either positive, negative, or neutral.\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7aec39",
   "metadata": {},
   "source": [
    "\n",
    "### Conetxt  Prompting\n",
    "- context provide LLMs what type of answers are desired.\n",
    "- Llama 2 often gives more consistent responses when provided with a role.\n",
    "- First, try standard prompt and see the respons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5010e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "How can I answer this question from my friend:\n",
    "What is the meaning of life?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1316f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Think you as a life coach \\\n",
    "who gives advice to people about living a good life.\\\n",
    "You attempt to provide unbiased advice.\n",
    "You respond in the soft tone\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "{context}\n",
    "How can I answer this question from my friend:\n",
    "What is the meaning of life?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a71ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"\"\"\n",
    "your role as a life coach \\\n",
    "who gives advice to people about living a good life.\\\n",
    "You attempt to provide unbiased advice.\n",
    "You respond in the soft tone\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "{role}\n",
    "How can I answer this question from my friend:\n",
    "What is the meaning of life?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"\"\"\n",
    "Think you are technical recruiter at Meta\n",
    "Am a candidate came for a Data Scientist Interview at Meta\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "{role}\n",
    "what are all the question you ask me?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf7f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"\"\"\n",
    "You are Data Scientist Tutor\n",
    "\"\"\"\n",
    "\n",
    "prompt=f\"\"\"\n",
    "{role}\n",
    "\n",
    "Project Toic : Manufacturing Industry\n",
    "\n",
    "User :\n",
    "1.Help me in creating a project and explain step by step on the analysis\n",
    "2.Write a python code and explain step by step\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b346de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load Boston housing dataset\n",
    "boston = load_boston()\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create and train linear regression model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on testing set\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "r2 = reg.score(X_test, y_test)\n",
    "print(\"R-squared value:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a94d2a",
   "metadata": {},
   "source": [
    "### Chain-of-thought Prompting\n",
    "- LLMs can perform better at reasoning and logic problems if you ask them to break the problem down into smaller steps. This is known as **chain-of-thought** prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eb3231",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "\n",
    "Think step by step\n",
    "\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3308a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "There  are 15 fruits in the basket, \n",
    "I had 3 bananas and 2 apples yesterday\n",
    "\n",
    "Today I bought 3 more apples\n",
    "\n",
    "How many fruits are currently in the basket?\n",
    "\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24e5570",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "\n",
    "Initially 14 fruits are in the basket, \n",
    "\n",
    "I had 3 bananas and 2 apples yesterday\n",
    "\n",
    "Today I bought 3 more apples\n",
    "\n",
    "How many fruits are currently in the basket?\n",
    "\n",
    "Think step by step \n",
    "\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad18543",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "\n",
    "Initially 14 fruits are in the basket, \n",
    "\n",
    "I had 3 bananas and 2 apples yesterday\n",
    "\n",
    "Today I bought 3 more apples\n",
    "\n",
    "How many fruits are currently in the basket?\n",
    "\n",
    "Think step by step \n",
    "\n",
    "\"\"\"\n",
    "response = llama(prompt,model=\"togethercomputer/llama-2-70b-chat\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab0e8a",
   "metadata": {},
   "source": [
    "# Code Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6fbc6f",
   "metadata": {},
   "source": [
    "### Here are the names of the Code Llama models provided by Together.ai:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b28e58",
   "metadata": {},
   "source": [
    "- ```togethercomputer/CodeLlama-7b```\n",
    "- ```togethercomputer/CodeLlama-13b```\n",
    "- ```togethercomputer/CodeLlama-34b```\n",
    "- ```togethercomputer/CodeLlama-7b-Python```\n",
    "- ```togethercomputer/CodeLlama-13b-Python```\n",
    "- ```togethercomputer/CodeLlama-34b-Python```\n",
    "- ```togethercomputer/CodeLlama-7b-Instruct```\n",
    "- ```togethercomputer/CodeLlama-13b-Instruct```\n",
    "- ```togethercomputer/CodeLlama-34b-Instruct```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524ed037",
   "metadata": {},
   "source": [
    "### Import helper functions\n",
    "\n",
    "- You can examine the code_llama helper function using the menu above and selections File -> Open -> utils.py.\n",
    "- By default, the `code_llama` functions uses the CodeLlama-7b-Instruct model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a712e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import llama, code_llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020cd83",
   "metadata": {},
   "source": [
    "### Writing code to solve a math problem\n",
    "\n",
    "Lists of daily minimum and maximum temperatures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9103c85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_min = [42, 52, 47, 47, 53, 48, 47, 53, 55, 56, 57, 50, 48, 45]\n",
    "temp_max = [55, 57, 59, 59, 58, 62, 65, 65, 64, 63, 60, 60, 62, 62]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae80b5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2469b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Ask the Llama 7B model to determine the day with the lowest temperature.\n",
    "prompt = f\"\"\"\n",
    "Below is the 14 day temperature forecast in fahrenheit degree:\n",
    "14-day low temperatures: {temp_min}\n",
    "14-day high temperatures: {temp_max}\n",
    "Which day has the lowest temperature?\n",
    "\"\"\"\n",
    "\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Ask the Llama 70B model to determine the day with the lowest temperature.\n",
    "prompt = f\"\"\"\n",
    "Below is the 14 day temperature forecast in fahrenheit degree:\n",
    "14-day low temperatures: {temp_min}\n",
    "14-day high temperatures: {temp_max}\n",
    "Which day has the lowest temperature?\n",
    "\"\"\"\n",
    "\n",
    "response = llama(prompt,model=\"togethercomputer/llama-2-70b-chat\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f7cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Ask the Llama 70B model to determine the day with the lowest temperature.\n",
    "prompt = f\"\"\"\n",
    "Below is the 14 day temperature forecast in fahrenheit degree:\n",
    "14-day low temperatures: {temp_min}\n",
    "14-day high temperatures: {temp_max}\n",
    "\n",
    "Which day has the lowest temperature?\n",
    "\n",
    "Which day has the highst temperature?\n",
    "\n",
    "Solve using python code and explain the code\n",
    "\"\"\"\n",
    "\n",
    "response = llama(prompt,model=\"togethercomputer/llama-2-70b-chat\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66115232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Ask the Llama 70B model to determine the day with the lowest temperature.\n",
    "prompt = f\"\"\"\n",
    "Below is the 14 day temperature forecast in fahrenheit degree:\n",
    "14-day low temperatures: {temp_min}\n",
    "14-day high temperatures: {temp_max}\n",
    "\n",
    "Which day has the lowest temperature?\n",
    "\n",
    "Which day has the highst temperature?\n",
    "\n",
    "Solve using java code and explain the code\n",
    "\"\"\"\n",
    "\n",
    "response = llama(prompt,model=\"togethercomputer/llama-2-70b-chat\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9819e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
